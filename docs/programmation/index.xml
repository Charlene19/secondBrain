<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programmations on My Zettelkasten</title>
    <link>https://charlene19.github.io/secondBrain/programmation/</link>
    <description>Recent content in Programmations on My Zettelkasten</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Wed, 31 Mar 2021 00:00:00 +0200</lastBuildDate><atom:link href="https://charlene19.github.io/secondBrain/programmation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Sentiment Analysis</title>
      <link>https://charlene19.github.io/secondBrain/programmation/20210331151745-sentimentanalysis/</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 +0200</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/programmation/20210331151745-sentimentanalysis/</guid>
      <description>Je vais reprendre l&amp;rsquo;appli de Aboullaite Med. Le principe est de faire un analyseur de sentiments en s&amp;rsquo;appuyant sur standfordCore Nlp : Cette bibliothèque de nlp pour Java. L&amp;rsquo;appli est sur deux volets. Un back-end : Avec spring boot Un frontend : ReactJs
Ma connaissance de ces deux technos vont me permettre de remanier ce programme qui est pour l&amp;rsquo;instant est fonctionnel. Mais qui contient plusieurs choses à améliorer : le front, l&amp;rsquo;analyse des sentiments n&amp;rsquo;est pas disponible en français avec la bibli.</description>
    </item>
    
    <item>
      <title>Premiere compétition Kaggle</title>
      <link>https://charlene19.github.io/secondBrain/programmation/20210322115253-premiere/</link>
      <pubDate>Mon, 22 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/programmation/20210322115253-premiere/</guid>
      <description>Les consignes :
 1- Build a Random Forest model with all of your data (X and y). 2- Read in the &amp;ldquo;test&amp;rdquo; data, which doesn&amp;rsquo;t include values for the target. Predict home values in the test data with your Random Forest model. 3- Submit those predictions to the competition and see your score. 4- Optionally, come back to see if you can improve your model by adding features or changing your model.</description>
    </item>
    
    <item>
      <title>GitHub Action : Tuto GitLab</title>
      <link>https://charlene19.github.io/secondBrain/programmation/20210319095430-githubaction/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/programmation/20210319095430-githubaction/</guid>
      <description>COurs sur les GitHub WorkFlow.
Il y a deux types d&amp;rsquo;actions : Container actions ou Js Actions.
 Docker container actions allow the environment to be packaged with the GitHub Actions code and can only execute in the GitHub-Hosted Linux environment.
JavaScript actions decouple the GitHub Actions code from the environment allowing faster execution but accepting greater dependency management responsibility.
  jobs: is the base component of a workflow run build: is the identifier we&amp;rsquo;re attaching to this job name: is the name of the job, this is displayed on GitHub when the workflow is running runs-on: defines the type of machine to run the job on.</description>
    </item>
    
    <item>
      <title>Projet &#39;Recognizer emotional&#39;</title>
      <link>https://charlene19.github.io/secondBrain/programmation/20210317141628-projet/</link>
      <pubDate>Wed, 17 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/programmation/20210317141628-projet/</guid>
      <description>Projet construit à partir de cet article. Je ne sais pour l&amp;rsquo;instant pas quelle liberté je prendrais par rapport à lui. Les librairies : Librosa, SoundFile, NumPy, Scikit-Learn.
En premier lieu, j&amp;rsquo;ai dû télécharger des samples de données, en l&amp;rsquo;occurence des voix sur kagle. Puis définir un dictionnaire de données dans mon fichier. Afin de tester et faire progresser au mieux mon programme, j&amp;rsquo;ai sélectionné 3 émotions.
emotion_labels = { &#39;01&#39;: &#39;neutral&#39;, &#39;02&#39;: &#39;calm&#39;, &#39;03&#39;: &#39;happy&#39;, &#39;04&#39;: &#39;sad&#39;, &#39;05&#39;: &#39;angry&#39;, &#39;06&#39;: &#39;fearful&#39;, &#39;07&#39;: &#39;disgust&#39;, &#39;08&#39;: &#39;surprised&#39; } focused_emotion_labels = [&#39;happy&#39;, &#39;sad&#39;, &#39;angry&#39;] Extraction</description>
    </item>
    
    <item>
      <title>Gist</title>
      <link>https://charlene19.github.io/secondBrain/programmation/20210317111323-gist/</link>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/programmation/20210317111323-gist/</guid>
      <description>Création d&amp;rsquo;un gist.
Il est possible de créer un gist public ou privé. Je vais laisser le scrapperPe en pv pour l&amp;rsquo;instant.
Il suffit juste d&amp;rsquo;aller sur l&amp;rsquo;onglet Gist de sa page gh et copier-coller son snippets et c&amp;rsquo;est bon.</description>
    </item>
    
    <item>
      <title>Crawler</title>
      <link>https://charlene19.github.io/secondBrain/programmation/20210310102636-crawler/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/programmation/20210310102636-crawler/</guid>
      <description>import requests from bs4 import BeautifulSoup class Crawler: def getPage(self, url): try: req = requests.get(url) except requests.exceptions.RequestException: return None return BeautifulSoup(req.text, &#39;html.parser&#39;) def safeGet(self, pageObj, selector): &amp;quot;&amp;quot;&amp;quot; Utilty function used to get a content string from a Beautiful Soup object and a selector. Returns an empty string if no object is found for the given selector &amp;quot;&amp;quot;&amp;quot; selectedElems = pageObj.select(selector) if selectedElems is not None and len(selectedElems) &amp;gt; 0: return &#39;\n&#39;.</description>
    </item>
    
    <item>
      <title>CrwalSpider</title>
      <link>https://charlene19.github.io/secondBrain/programmation/20210310135927-crwalspider/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/programmation/20210310135927-crwalspider/</guid>
      <description>from scrapy.contrib.linkextractors import LinkExtractor from scrapy.contrib.spiders import CrawlSpider, Rule class ArticleSpider(CrawlSpider): name = &#39;articles&#39; allowed_domains = [&#39;wikipedia.org&#39;] start_urls = [&#39;https://en.wikipedia.org/wiki/&#39; &#39;Benevolent_dictator_for_life&#39;] rules = [Rule(LinkExtractor(allow=r&#39;.*&#39;), callback=&#39;parse_items&#39;, follow=True)] def parse_items(self, response): url = response.url title = response.css(&#39;h1::text&#39;).extract_first() text = response.xpath(&#39;//div[@id=&amp;quot;mw-content-text&amp;quot;]//text()&#39;) .extract() lastUpdated = response.css(&#39;li#footer-info-lastmod::text&#39;) .extract_first() lastUpdated = lastUpdated.replace( &#39;This page was last edited on &#39;, &#39;&#39;) print(&#39;URL is: {}&#39;.format(url)) print(&#39;title is: {} &#39;.format(title)) print(&#39;text is: {}&#39;.format(text)) print(&#39;Last updated: {}&#39;.format(lastUpdated)) #scrapy.contrib est depreacted : Nouvelle location</description>
    </item>
    
    <item>
      <title>Domain Format</title>
      <link>https://charlene19.github.io/secondBrain/programmation/20210309135210-domain_format/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/programmation/20210309135210-domain_format/</guid>
      <description>domain = &#39;{}://{}&#39;.format(urlparse(siteUrl).scheme, urlparse(siteUrl).netloc) </description>
    </item>
    
    <item>
      <title>Scrappy</title>
      <link>https://charlene19.github.io/secondBrain/programmation/20210310110014-scrappy/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/programmation/20210310110014-scrappy/</guid>
      <description>spider : est un projet Scrapy
 For large sites with many types of content, you might have separate Scrapy items for each type (blog posts, press releases, articles, etc.), each with different fields, but all running under the same Scrapy project. The name of each spider must be unique within the project.
 Deux méthodes à noter :
def start_requests(self): urls = [ &#39;http://en.wikipedia.org/wiki/Python_&#39; &#39;%28programming_language%29&#39;, &#39;https://en.wikipedia.org/wiki/Functional_programming&#39;, &#39;https://en.wikipedia.org/wiki/Monty_Python&#39;] return [scrapy.Request(url=url, callback=self.</description>
    </item>
    
    <item>
      <title>Selenium</title>
      <link>https://charlene19.github.io/secondBrain/programmation/20210311163747-selenium/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/programmation/20210311163747-selenium/</guid>
      <description>Selenium va me permettre de scrapper du Js.
J&amp;rsquo;ai installé le package via IntelliJ sur mon env. Mais je dois encore lui offrir un accès à mon browser. Ici, google chrome :
Description browser Selenium
J&amp;rsquo;ai importer via PyCharm : Selenium. Puis dwl les drivers chrome.
J&amp;rsquo;ai dû ouvrir les droits :
sudo chmod u+rwxX /usr/bin/chromedriver Maintenant à appliquer avec des données retour de Form.</description>
    </item>
    
  </channel>
</rss>
