<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>scraping on My Zettelkasten</title>
    <link>https://charlene19.github.io/secondBrain/tags/scraping/</link>
    <description>Recent content in scraping on My Zettelkasten</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Tue, 09 Mar 2021 00:00:00 +0100</lastBuildDate><atom:link href="https://charlene19.github.io/secondBrain/tags/scraping/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Crawler</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210310102636-crawler/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210310102636-crawler/</guid>
      <description>import requests from bs4 import BeautifulSoup class Crawler: def getPage(self, url): try: req = requests.get(url) except requests.exceptions.RequestException: return None return BeautifulSoup(req.text, &#39;html.parser&#39;) def safeGet(self, pageObj, selector): &amp;quot;&amp;quot;&amp;quot; Utilty function used to get a content string from a Beautiful Soup object and a selector. Returns an empty string if no object is found for the given selector &amp;quot;&amp;quot;&amp;quot; selectedElems = pageObj.select(selector) if selectedElems is not None and len(selectedElems) &amp;gt; 0: return &#39;\n&#39;.</description>
    </item>
    
    <item>
      <title>CrwalSpider</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210310135927-crwalspider/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210310135927-crwalspider/</guid>
      <description>from scrapy.contrib.linkextractors import LinkExtractor from scrapy.contrib.spiders import CrawlSpider, Rule class ArticleSpider(CrawlSpider): name = &#39;articles&#39; allowed_domains = [&#39;wikipedia.org&#39;] start_urls = [&#39;https://en.wikipedia.org/wiki/&#39; &#39;Benevolent_dictator_for_life&#39;] rules = [Rule(LinkExtractor(allow=r&#39;.*&#39;), callback=&#39;parse_items&#39;, follow=True)] def parse_items(self, response): url = response.url title = response.css(&#39;h1::text&#39;).extract_first() text = response.xpath(&#39;//div[@id=&amp;quot;mw-content-text&amp;quot;]//text()&#39;) .extract() lastUpdated = response.css(&#39;li#footer-info-lastmod::text&#39;) .extract_first() lastUpdated = lastUpdated.replace( &#39;This page was last edited on &#39;, &#39;&#39;) print(&#39;URL is: {}&#39;.format(url)) print(&#39;title is: {} &#39;.format(title)) print(&#39;text is: {}&#39;.format(text)) print(&#39;Last updated: {}&#39;.format(lastUpdated)) #scrapy.contrib est depreacted : Nouvelle location</description>
    </item>
    
    <item>
      <title>Domain Format</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210309135210-domain_format/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210309135210-domain_format/</guid>
      <description>domain = &#39;{}://{}&#39;.format(urlparse(siteUrl).scheme, urlparse(siteUrl).netloc) </description>
    </item>
    
    <item>
      <title>Scrappy</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210310110014-scrappy/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210310110014-scrappy/</guid>
      <description>spider : est un projet Scrapy
 For large sites with many types of content, you might have separate Scrapy items for each type (blog posts, press releases, articles, etc.), each with different fields, but all running under the same Scrapy project. The name of each spider must be unique within the project.
 Deux méthodes à noter :
def start_requests(self): urls = [ &#39;http://en.wikipedia.org/wiki/Python_&#39; &#39;%28programming_language%29&#39;, &#39;https://en.wikipedia.org/wiki/Functional_programming&#39;, &#39;https://en.wikipedia.org/wiki/Monty_Python&#39;] return [scrapy.Request(url=url, callback=self.</description>
    </item>
    
    <item>
      <title>Selenium</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210311163747-selenium/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210311163747-selenium/</guid>
      <description>Selenium va me permettre de scrapper du Js.
J&amp;rsquo;ai installé le package via IntelliJ sur mon env. Mais je dois encore lui offrir un accès à mon browser. Ici, google chrome :
Description browser Selenium
J&amp;rsquo;ai importer via PyCharm : Selenium. Puis dwl les drivers chrome.
J&amp;rsquo;ai dû ouvrir les droits :
sudo chmod u+rwxX /usr/bin/chromedriver Maintenant à appliquer avec des données retour de Form.</description>
    </item>
    
  </channel>
</rss>
