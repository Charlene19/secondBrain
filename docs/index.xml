<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My Zettelkasten</title>
    <link>https://charlene19.github.io/secondBrain/</link>
    <description>Recent content on My Zettelkasten</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Tue, 30 Mar 2021 00:00:00 +0200</lastBuildDate><atom:link href="https://charlene19.github.io/secondBrain/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ash</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210330091948-ash/</link>
      <pubDate>Tue, 30 Mar 2021 00:00:00 +0200</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210330091948-ash/</guid>
      <description>Je vais commencer une série d&amp;rsquo;analyse à propos des écrits de Ashley Morris. Ash est un blogger des années 2000 qui vivaient à la Nouvelle-Orléans et qui s&amp;rsquo;est fait connaître pour ses célebres coup de gueule. Son personnage a été repris dans la série Treme.
Les articles d&amp;rsquo;Ash Morris ne sont vraiment pas consensuels. C&amp;rsquo;est d&amp;rsquo;ailleurs pour ça, je pense, qu&amp;rsquo;il a eu un tel succès. Petu-être aussi à cause de la rareté des blogs à cette époque.</description>
    </item>
    
    <item>
      <title>Silvanus Thompson</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210330093307-silvanusthompson/</link>
      <pubDate>Tue, 30 Mar 2021 00:00:00 +0200</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210330093307-silvanusthompson/</guid>
      <description>Thompson fait partie de ces découvertes qui rendent plaisante l&amp;rsquo;apprentissage des maths. Tout comme Mr Rouget ou Ivan Savov, je trouve cette pédagogie d&amp;rsquo;apprentissage très plaisante. Et efficace contrairement à wikiversité qui est vraiment mal foutue pour les maths.
Calculus Made Easy est encore plus intéressant parce qu&amp;rsquo;il y a un aspect philosophique dans le méta-apprentissage. Mon propos n&amp;rsquo;est pas clair mais peut être illustrer par le prologue :</description>
    </item>
    
    <item>
      <title>Premiere compétition Kaggle</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210322115253-premiere/</link>
      <pubDate>Mon, 22 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210322115253-premiere/</guid>
      <description>Les consignes :
 1- Build a Random Forest model with all of your data (X and y). 2- Read in the &amp;ldquo;test&amp;rdquo; data, which doesn&amp;rsquo;t include values for the target. Predict home values in the test data with your Random Forest model. 3- Submit those predictions to the competition and see your score. 4- Optionally, come back to see if you can improve your model by adding features or changing your model.</description>
    </item>
    
    <item>
      <title>Stiegler</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210320094405-stiegler/</link>
      <pubDate>Fri, 19 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210320094405-stiegler/</guid>
      <description>Suite à l&amp;rsquo;écoute des deux interviews de Stiegler chez Thinkerview, j&amp;rsquo;ai repéré deux choses. La première, j&amp;rsquo;aimerais me rapprocher de son institut de recherche scientifique. Je n&amp;rsquo;ai pas très bien compris ce qui s&amp;rsquo;y fait. Mais l&amp;rsquo;idée d&amp;rsquo;utilisation de datas pour redonner la place est intéressante. Il est possible que ça soit une coquille vide mais quand même.
La deuxième chose est : Le ou les jardins de Pierre :</description>
    </item>
    
    <item>
      <title>GitHub Action : Tuto GitLab</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210319095430-githubaction/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210319095430-githubaction/</guid>
      <description>COurs sur les GitHub WorkFlow.
Il y a deux types d&amp;rsquo;actions : Container actions ou Js Actions.
 Docker container actions allow the environment to be packaged with the GitHub Actions code and can only execute in the GitHub-Hosted Linux environment.
JavaScript actions decouple the GitHub Actions code from the environment allowing faster execution but accepting greater dependency management responsibility.
  jobs: is the base component of a workflow run build: is the identifier we&amp;rsquo;re attaching to this job name: is the name of the job, this is displayed on GitHub when the workflow is running runs-on: defines the type of machine to run the job on.</description>
    </item>
    
    <item>
      <title>Projet &#39;Recognizer emotional&#39;</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210317141628-projet/</link>
      <pubDate>Wed, 17 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210317141628-projet/</guid>
      <description>Projet construit à partir de cet article. Je ne sais pour l&amp;rsquo;instant pas quelle liberté je prendrais par rapport à lui. Les librairies : Librosa, SoundFile, NumPy, Scikit-Learn.
En premier lieu, j&amp;rsquo;ai dû télécharger des samples de données, en l&amp;rsquo;occurence des voix sur kagle. Puis définir un dictionnaire de données dans mon fichier. Afin de tester et faire progresser au mieux mon programme, j&amp;rsquo;ai sélectionné 3 émotions.
emotion_labels = { &#39;01&#39;: &#39;neutral&#39;, &#39;02&#39;: &#39;calm&#39;, &#39;03&#39;: &#39;happy&#39;, &#39;04&#39;: &#39;sad&#39;, &#39;05&#39;: &#39;angry&#39;, &#39;06&#39;: &#39;fearful&#39;, &#39;07&#39;: &#39;disgust&#39;, &#39;08&#39;: &#39;surprised&#39; } focused_emotion_labels = [&#39;happy&#39;, &#39;sad&#39;, &#39;angry&#39;] Extraction</description>
    </item>
    
    <item>
      <title>Gist</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210317111323-gist/</link>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210317111323-gist/</guid>
      <description>Création d&amp;rsquo;un gist.
Il est possible de créer un gist public ou privé. Je vais laisser le scrapperPe en pv pour l&amp;rsquo;instant.
Il suffit juste d&amp;rsquo;aller sur l&amp;rsquo;onglet Gist de sa page gh et copier-coller son snippets et c&amp;rsquo;est bon.</description>
    </item>
    
    <item>
      <title>Crawler</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210310102636-crawler/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210310102636-crawler/</guid>
      <description>import requests from bs4 import BeautifulSoup class Crawler: def getPage(self, url): try: req = requests.get(url) except requests.exceptions.RequestException: return None return BeautifulSoup(req.text, &#39;html.parser&#39;) def safeGet(self, pageObj, selector): &amp;quot;&amp;quot;&amp;quot; Utilty function used to get a content string from a Beautiful Soup object and a selector. Returns an empty string if no object is found for the given selector &amp;quot;&amp;quot;&amp;quot; selectedElems = pageObj.select(selector) if selectedElems is not None and len(selectedElems) &amp;gt; 0: return &#39;\n&#39;.</description>
    </item>
    
    <item>
      <title>CrwalSpider</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210310135927-crwalspider/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210310135927-crwalspider/</guid>
      <description>from scrapy.contrib.linkextractors import LinkExtractor from scrapy.contrib.spiders import CrawlSpider, Rule class ArticleSpider(CrawlSpider): name = &#39;articles&#39; allowed_domains = [&#39;wikipedia.org&#39;] start_urls = [&#39;https://en.wikipedia.org/wiki/&#39; &#39;Benevolent_dictator_for_life&#39;] rules = [Rule(LinkExtractor(allow=r&#39;.*&#39;), callback=&#39;parse_items&#39;, follow=True)] def parse_items(self, response): url = response.url title = response.css(&#39;h1::text&#39;).extract_first() text = response.xpath(&#39;//div[@id=&amp;quot;mw-content-text&amp;quot;]//text()&#39;) .extract() lastUpdated = response.css(&#39;li#footer-info-lastmod::text&#39;) .extract_first() lastUpdated = lastUpdated.replace( &#39;This page was last edited on &#39;, &#39;&#39;) print(&#39;URL is: {}&#39;.format(url)) print(&#39;title is: {} &#39;.format(title)) print(&#39;text is: {}&#39;.format(text)) print(&#39;Last updated: {}&#39;.format(lastUpdated)) #scrapy.contrib est depreacted : Nouvelle location</description>
    </item>
    
    <item>
      <title>Domain Format</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210309135210-domain_format/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210309135210-domain_format/</guid>
      <description>domain = &#39;{}://{}&#39;.format(urlparse(siteUrl).scheme, urlparse(siteUrl).netloc) </description>
    </item>
    
  </channel>
</rss>
