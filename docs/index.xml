<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My Zettelkasten</title>
    <link>https://charlene19.github.io/secondBrain/</link>
    <description>Recent content on My Zettelkasten</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Tue, 09 Mar 2021 00:00:00 +0100</lastBuildDate><atom:link href="https://charlene19.github.io/secondBrain/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Crawler</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210310102636-crawler/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210310102636-crawler/</guid>
      <description>import requests from bs4 import BeautifulSoup class Crawler: def getPage(self, url): try: req = requests.get(url) except requests.exceptions.RequestException: return None return BeautifulSoup(req.text, &#39;html.parser&#39;) def safeGet(self, pageObj, selector): &amp;quot;&amp;quot;&amp;quot; Utilty function used to get a content string from a Beautiful Soup object and a selector. Returns an empty string if no object is found for the given selector &amp;quot;&amp;quot;&amp;quot; selectedElems = pageObj.select(selector) if selectedElems is not None and len(selectedElems) &amp;gt; 0: return &#39;\n&#39;.</description>
    </item>
    
    <item>
      <title>CrwalSpider</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210310135927-crwalspider/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210310135927-crwalspider/</guid>
      <description>from scrapy.contrib.linkextractors import LinkExtractor from scrapy.contrib.spiders import CrawlSpider, Rule class ArticleSpider(CrawlSpider): name = &#39;articles&#39; allowed_domains = [&#39;wikipedia.org&#39;] start_urls = [&#39;https://en.wikipedia.org/wiki/&#39; &#39;Benevolent_dictator_for_life&#39;] rules = [Rule(LinkExtractor(allow=r&#39;.*&#39;), callback=&#39;parse_items&#39;, follow=True)] def parse_items(self, response): url = response.url title = response.css(&#39;h1::text&#39;).extract_first() text = response.xpath(&#39;//div[@id=&amp;quot;mw-content-text&amp;quot;]//text()&#39;) .extract() lastUpdated = response.css(&#39;li#footer-info-lastmod::text&#39;) .extract_first() lastUpdated = lastUpdated.replace( &#39;This page was last edited on &#39;, &#39;&#39;) print(&#39;URL is: {}&#39;.format(url)) print(&#39;title is: {} &#39;.format(title)) print(&#39;text is: {}&#39;.format(text)) print(&#39;Last updated: {}&#39;.format(lastUpdated)) #scrapy.contrib est depreacted : Nouvelle location</description>
    </item>
    
    <item>
      <title>Domain Format</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210309135210-domain_format/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210309135210-domain_format/</guid>
      <description>domain = &#39;{}://{}&#39;.format(urlparse(siteUrl).scheme, urlparse(siteUrl).netloc) </description>
    </item>
    
    <item>
      <title>Scrappy</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210310110014-scrappy/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210310110014-scrappy/</guid>
      <description>spider : est un projet Scrapy
 For large sites with many types of content, you might have separate Scrapy items for each type (blog posts, press releases, articles, etc.), each with different fields, but all running under the same Scrapy project. The name of each spider must be unique within the project.
 Deux méthodes à noter :
def start_requests(self): urls = [ &#39;http://en.wikipedia.org/wiki/Python_&#39; &#39;%28programming_language%29&#39;, &#39;https://en.wikipedia.org/wiki/Functional_programming&#39;, &#39;https://en.wikipedia.org/wiki/Monty_Python&#39;] return [scrapy.Request(url=url, callback=self.</description>
    </item>
    
    <item>
      <title>Selenium</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210311163747-selenium/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210311163747-selenium/</guid>
      <description>Selenium va me permettre de scrapper du Js.
J&amp;rsquo;ai installé le package via IntelliJ sur mon env. Mais je dois encore lui offrir un accès à mon browser. Ici, google chrome :
Description browser Selenium
J&amp;rsquo;ai importer via PyCharm : Selenium. Puis dwl les drivers chrome.
J&amp;rsquo;ai dû ouvrir les droits :
sudo chmod u+rwxX /usr/bin/chromedriver Maintenant à appliquer avec des données retour de Form.</description>
    </item>
    
    <item>
      <title>About me</title>
      <link>https://charlene19.github.io/secondBrain/about-hugo/</link>
      <pubDate>Sun, 07 Mar 2021 19:47:46 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/about-hugo/</guid>
      <description>Je suis, quelque part par là :
Et à peu près comme ça :</description>
    </item>
    
    <item>
      <title>Bézier Courbe en ReactJs</title>
      <link>https://charlene19.github.io/secondBrain/posts/beziercurve/</link>
      <pubDate>Sat, 06 Mar 2021 19:47:46 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/beziercurve/</guid>
      <description>Un tracé est d&#39;abord initialisé par la méthode beginPath(). Le point de référence de début du tracé est désigné avec moveTo(x,y). Il s&#39;agit en quelque sorte de décider à partir de quel emplacement le pinceau va être posé. Puis vient le tracé de la ligne elle-même avec la méthode lineTo(x,y) qui va ajouter un segment au chemin qui fut débuté par beginPath(). On peut ajouter autant de segments que l&#39;on veut, puis éventuellement &amp;quot;fermer&amp;quot; la forme pour revenir automatiquement au point de départ avec closePath() .</description>
    </item>
    
    <item>
      <title>Questions débutant Java</title>
      <link>https://charlene19.github.io/secondBrain/posts/quesjava/</link>
      <pubDate>Sat, 06 Mar 2021 19:47:46 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/quesjava/</guid>
      <description>[Quelle est la différence entre : « Double » et « double » pour un attribut ? Quand mettre l&amp;rsquo;un ou l&amp;rsquo;autre ?] Le constructeur Double permet de définir un objet Double à partir d&amp;rsquo;une variable double :
public Double(double value);
[Qu&amp;rsquo;est-ce qu&amp;rsquo;un constructeur ?] Pour instancier une classe, c&#39;est-à-dire créer un objet à partir d&#39;une classe, il s&#39;agit d&#39;utiliser l&#39;opérateur new.
En réalité l&#39;opérateur new, lorsqu&#39;il est utilisé, fait appel à une méthode spéciale de la classe: le constructeur.</description>
    </item>
    
    <item>
      <title>Regex</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210309105325-regex/</link>
      <pubDate>Sat, 06 Mar 2021 00:00:00 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210309105325-regex/</guid>
      <description>[[./regx1.png]
[[./regx2.png]
[[./regx3.png]
Le cours sur les regex en Java est vraiment bien fait mais il me semble qu&amp;rsquo;il y en a un autre généraliste.
Par exemple sur le code wikiCrawlers :
L&amp;rsquo;auteur1 dit qu&amp;rsquo;il y a trois choses en communs :
 • They reside within the div with the id set to bodyContent . • The URLs do not contain colons. • The URLs begin with wiki.
 Ce qui nous permet de définir notre regex :</description>
    </item>
    
    <item>
      <title>Loss</title>
      <link>https://charlene19.github.io/secondBrain/posts/20210305103616-loss/</link>
      <pubDate>Mon, 01 Mar 2021 19:47:46 +0100</pubDate>
      
      <guid>https://charlene19.github.io/secondBrain/posts/20210305103616-loss/</guid>
      <description>:MachineLearning:
En apprentissage supervisé, c&amp;rsquo;est la notion de perte d&amp;rsquo;information qui est très imortante. Puisque c&amp;rsquo;est cette perte qu&amp;rsquo;il va falloir ramener au minimum.
 L&amp;rsquo;apprentissage se résume en fait souvent à une méthode itérative qui converge vers un minimum de cette fonction.
 L&amp;rsquo;erreur ou le rique
L&amp;rsquo;erreur quadratique, c&amp;rsquo;est la distance euclidienne entre un point et un modèle.
Le risque empirique : c&amp;rsquo;est le calcul de la somme de toutes les erreurs entre la résultante et le modèle.</description>
    </item>
    
  </channel>
</rss>
