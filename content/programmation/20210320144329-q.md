+++
title = "Q Learning"
author = ["Charlène"]
date = 2020-03-21
tags = ["MachineLearning"]
draft = false
+++

Technique d'apprentissage qui ne nécessite aucun modèle initial de l'environnement.

> Un des points forts du {\displaystyle Q}Q-learning est qu'il permet de comparer les récompenses probables de prendre les actions accessibles sans avoir de connaissance initiale de l’environnement. En d'autres termes, bien que le système soit modélisé par un processus de décision markovien (fini), l'agent qui apprend ne le connait pas et l'algorithme du {\displaystyle Q}Q-learning ne l'utilise pas.

Définition formelle :

> initialiser Q[s, a] pour tout état s, toute action a de façon arbitraire, mais Q(état terminal, a) = 0 pour tout action a
>
> répéter
>       //début d'un épisode
>       initialiser l'état s
>
> répéter
> 	 //étape d'un épisode
> 	 choisir une action a depuis s en utilisant la politique spécifiée par Q (par exemple ε-greedy)
> 	 exécuter l'action a
> 	 observer la récompense r et l'état s'
>
> Q[s, a] := Q[s, a] + α[r + γ maxa' Q(s', a') - Q(s, a)]
>
> 	 s := s'
> 	 a := a'
> jusqu'à ce que s soit l'état terminal

Il existe le double Q Learning

> Comme le Q-learning utilise l'estimateur max, le Q-learning surestime la valeur des actions et de fait, dans des environnements bruités, l'apprentissage est lent. Ce problème est résolu dans la variante appelée double Q-learning8 qui utilise deux fonctions d'évaluation {\displaystyle Q<sup>A</sup>}{\displaystyle Q<sup>A</sup>}et {\displaystyle Q<sup>B</sup>}{\displaystyle Q<sup>B</sup>}apprises sur deux ensembles d'expériences différents. La mise à jour se fait de façon croisée :
>
> {\displaystyle Q<sub>t+1</sub><sup>A</sup>(s<sub>t</sub>,a<sub>t</sub>)=Q<sub>t</sub><sup>A</sup>(s<sub>t</sub>,a<sub>t</sub>)+&alpha; \_{t}(s<sub>t</sub>,a<sub>t</sub>)\left(r<sub>t</sub>+&gamma; ~Q<sub>t</sub><sup>B</sup>\left(s<sub>t+1</sub>,\mathop {\operatorname {arg~max} } \_{a}Q<sub>t</sub><sup>A</sup>(s<sub>t+1</sub>,a)\right)-Q<sub>t</sub><sup>A</sup>(s<sub>t</sub>,a<sub>t</sub>)\right)}{\displaystyle Q<sub>t+1</sub><sup>A</sup>(s<sub>t</sub>,a<sub>t</sub>)=Q<sub>t</sub><sup>A</sup>(s<sub>t</sub>,a<sub>t</sub>)+&alpha; \_{t}(s<sub>t</sub>,a<sub>t</sub>)\left(r<sub>t</sub>+&gamma; ~Q<sub>t</sub><sup>B</sup>\left(s<sub>t+1</sub>,\mathop {\operatorname {arg~max} } \_{a}Q<sub>t</sub><sup>A</sup>(s<sub>t+1</sub>,a)\right)-Q<sub>t</sub><sup>A</sup>(s<sub>t</sub>,a<sub>t</sub>)\right)}, et
> {\displaystyle Q<sub>t+1</sub><sup>B</sup>(s<sub>t</sub>,a<sub>t</sub>)=Q<sub>t</sub><sup>B</sup>(s<sub>t</sub>,a<sub>t</sub>)+&alpha; \_{t}(s<sub>t</sub>,a<sub>t</sub>)\left(r<sub>t</sub>+&gamma; ~Q<sub>t</sub><sup>A</sup>\left(s<sub>t+1</sub>,\mathop {\operatorname {arg~max} } \_{a}Q<sub>t</sub><sup>B</sup>(s<sub>t+1</sub>,a)\right)-Q<sub>t</sub><sup>B</sup>(s<sub>t</sub>,a<sub>t</sub>)\right).}{\displaystyle Q<sub>t+1</sub><sup>B</sup>(s<sub>t</sub>,a<sub>t</sub>)=Q<sub>t</sub><sup>B</sup>(s<sub>t</sub>,a<sub>t</sub>)+&alpha; \_{t}(s<sub>t</sub>,a<sub>t</sub>)\left(r<sub>t</sub>+&gamma; ~Q<sub>t</sub><sup>A</sup>\left(s<sub>t+1</sub>,\mathop {\operatorname {arg~max} } \_{a}Q<sub>t</sub><sup>B</sup>(s<sub>t+1</sub>,a)\right)-Q<sub>t</sub><sup>B</sup>(s<sub>t</sub>,a<sub>t</sub>)\right).}
> Comme la valeur estimée est évaluée en utilisant une autre politique, le problème de la surestimation est résolu. L'apprentissage de l'algorithme à ensemble peut être effectué en utilisant des techniques d'apprentissage profond, ce qui donne les réseaux DQN (deep Q-networks, réseaux profonds Q). On peut alors avoir le Double DQN, pour obtenir de meilleures performances qu'avec l'algorithme DQN original9.

Notes :

La balance du Q Learning oscille entre 0 et 1. À 0, l'agent n'apprend rien et à 1 (qui est la valeur optimale) l'agent agit sans apprentissage des différents états. (à vérifier).
