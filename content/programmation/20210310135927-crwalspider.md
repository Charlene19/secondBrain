+++
title = "CrwalSpider"
author = ["Charlène"]
date = 2021-03-09
tags = ["scraping", "Python"]
draft = false
+++

```nil
from scrapy.contrib.linkextractors import LinkExtractor
from scrapy.contrib.spiders import CrawlSpider, Rule
class ArticleSpider(CrawlSpider):
name = 'articles'
allowed_domains = ['wikipedia.org']
start_urls = ['https://en.wikipedia.org/wiki/'
'Benevolent_dictator_for_life']
rules = [Rule(LinkExtractor(allow=r'.*'), callback='parse_items',
follow=True)]
def parse_items(self, response):
url = response.url
title = response.css('h1::text').extract_first()
text = response.xpath('//div[@id="mw-content-text"]//text()')
.extract()
lastUpdated = response.css('li#footer-info-lastmod::text')
.extract_first()
lastUpdated = lastUpdated.replace(
'This page was last edited on ', '')
print('URL is: {}'.format(url))
print('title is: {} '.format(title))
print('text is: {}'.format(text))
print('Last updated: {}'.format(lastUpdated))
```

\#scrapy.contrib est depreacted : [Nouvelle location](https://docs.scrapy.org/en/latest/news.html#full-list-of-relocations)

Scrapy permet de générer les datas sous différents formats :

```nil
$ scrapy runspider articleItems.py -o articles.csv -t csv
$ scrapy runspider articleItems.py -o articles.json -t json
$ scrapy runspider articleItems.py -o articles.xml -t xml
```

[itemPipeline](20210310141632-itempipeline.org)Pour aller plus loin avec Scrapy : **Learning Scrapy: Learn the art of eficient web scraping and crawling with Python
Kouzis-Loukas Dimitrios**
