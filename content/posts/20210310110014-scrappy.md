+++
title = "Scrappy"
author = ["Charlène"]
date = 2021-03-09
tags = ["scraping", "Python"]
draft = false
+++

spider : est un projet Scrapy

> For large sites with many types of content, you might have separate Scrapy items for
> each type (blog posts, press releases, articles, etc.), each with different fields, but all
> running under the same Scrapy project. The name of each spider must be unique
> within the project.

Deux méthodes à noter :

```nil
def start_requests(self):
    urls = [
	'http://en.wikipedia.org/wiki/Python_'
	'%28programming_language%29',
	'https://en.wikipedia.org/wiki/Functional_programming',
	'https://en.wikipedia.org/wiki/Monty_Python']
    return [scrapy.Request(url=url, callback=self.parse)
	    for url in urls]
```

Methode d'entrée du programme. Elle fournit la request.

```nil
def parse(self, response):
       url = response.url
       title = response.css('h1::text').extract_first()
       print('URL is: {}'.format(url))
       print('Title is: {}'.format(title))
```

[CrwalSpider](20210310135927-crwalspider.org)

Scrapy permet de faire des objets qui faciliteront le stockage.

```nil
from scrapy.contrib.linkextractors import LinkExtractor
from scrapy.contrib.spiders import CrawlSpider, Rule
from wikiSpider.items import Article
class ArticleSpider(CrawlSpider):
name = 'articleItems'
allowed_domains = ['wikipedia.org']
start_urls = ['https://en.wikipedia.org/wiki/Benevolent'
'_dictator_for_life']
rules = [
Rule(LinkExtractor(allow='(/wiki/)((?!:).)*$'),
callback='parse_items', follow=True),
]
def parse_items(self, response):
article = Article()
article['url'] = response.url
article['title'] = response.css('h1::text').extract_first()
article['text'] = response.xpath('//div[@id='
'"mw-content-text"]//text()').extract()
lastUpdated = response.css('li#footer-info-lastmod::text')
.extract_first()
article['lastUpdated'] = lastUpdated.replace('This page was '
'last edited on ', '')
return article
```
