+++
title = "Loss"
author = ["Charlène"]
date = 2021-03-01
tags = ["MachineLearning"]
draft = false
+++

:MachineLearning:

En apprentissage supervisé, c'est la notion de perte d'information qui est très imortante. Puisque c'est cette perte qu'il va falloir ramener au minimum.

> L'apprentissage se résume en fait souvent à une méthode itérative qui converge vers un minimum de cette fonction.

**L'erreur ou le rique**

L'erreur quadratique, c'est la distance euclidienne entre un point et un modèle.

Le risque empirique : c'est le calcul de la somme de toutes les erreurs entre la résultante et le modèle.
[statistical learning theory ]

**La vraissemblance**

Ici le but de l'algorithme est de converger vers le maximum de la fonction de la vraisemblance.

Ô : est un estimateur.

[Lien entre Loss et Vraissemblance](https://quantivity.wordpress.com/2011/05/23/why-minimize-negative-log-likelihood/)
